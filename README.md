# Word Segmentation for No-Space Texts

## Задача
В пользовательских текстах (поиск, описания, заголовки) часто встречаются опечатки и слитное написание слов:  книгавхорошемсостоянии


Цель: **восстановить позиции пропущенных пробелов**, чтобы улучшить:
- понимание текста поисковыми системами и рекомендательными алгоритмами;
- обработку пользовательского ввода (поиск, формы, чат);
- подготовку данных для downstream NLP-задач (ассистенты, извлечение сущностей, классификация и т.д.).

Метрика качества: **средний F1-score** по позициям вставленных пробелов.  

---

## Подход

### 1. Словарь
- Словарь строится по корпусу WB (описания товаров).  
- Используются частотные слова, приводимые к единому регистру и очищенные от лишних символов.  

### 2. Алгоритм
- Применяется классический алгоритм **динамического программирования (DP)**:  
  - Строка без пробелов разбивается на токены с минимальной «стоимостью».  
  - Стоимость = вероятность/частота слова (чем чаще встречается слово в словаре, тем ниже стоимость).  
- Восстановленные позиции пробелов = границы между найденными токенами.  

### 3. Метрика
- Для каждого текста сравниваются предсказанные и истинные позиции пробелов.  
- Считаются precision, recall и F1.  

---

## Результаты

- На WB (валидация): **F1 ≈ 0.78**.  
- Precision выше Recall: алгоритм осторожный, вставляет пробелы в основном корректно, но часть пропусков остаётся.  
- Увеличение словаря не дало значимого прироста — полезные слова уже покрыты.  
- Раздувание словаря до миллионов элементов только вносит шум и не улучшает метрику.  

---

## Ключевые выводы
- Чистое DP с достаточно богатым словарём даёт стабильный и воспроизводимый результат.  
- Для задачи уровня маркетплейс-поиска этого качества достаточно, чтобы улучшить recall поиска и работу рекомендательных алгоритмов.  
- Простота решения = лёгкость поддержки и высокая скорость работы.  

---

## Другие подходы

Я также экспериментировал с альтернативами:

1. **BiGRU** — модель обучалась на символах предсказывать границы слов.  
   - На WB показывала очень высокий F1 (>0.98).  
   - Но переносимость на Avito оказалась хуже, чем у чистого DP.

2. **Комбинация BiGRU + DP** — идея была в том, чтобы BiGRU исправляла ошибки DP.  
   - На практике качество только падало: BiGRU ломала предсказуемость DP даже при осторожных порогах.  

---

## Почему без больших моделей?
Я сознательно не стал использовать крупные LLM и трансформеры:  
- Для такой простой задачи они **избыточны**.  
- Решение должно быть **лёгким и быстрым** для запуска.  
- Чистое DP закрывает задачу на достаточном уровне качества.  

---

## Примечание
В .zip архиве находится словарь. Настоятельно рекомендую при проверке распаковать архив и использовать полученный словарь, чтобы проверка не заняла слишком много времени.
